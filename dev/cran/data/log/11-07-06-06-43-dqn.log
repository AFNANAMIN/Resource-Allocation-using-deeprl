band            : 10000000.0
buffer_size     : 100000
demand_max      : 20000000.0
demand_min      : 10000000.0
dir_log         : ./dev\cran\data\log
dir_mod         : ./dev\cran\data\mod
dir_sum         : ./dev\cran\data\sum
episodes        : 100
epochs          : 3
epsilon_final   : 0.01
epsilon_init    : 0.5
epsilon_steps   : 4000
eta             : 0.25
gamma           : 0.99
load_id         : None
lr              : 0.001
mini_batch      : 64
num_rrh         : 6
num_usr         : 3
observations    : 100
pow_gap         : 3.0
pow_on          : 6.8
pow_slp         : 4.3
pow_tsm         : 1.0
random_seed     : 10000
run_id          : 11-07-06-06-43
save_ep         : 20
tests           : 20
theta_2         : 6.3095734448e-14
tm              : 1
update          : 8
| Episode: 20 | Demand: [1.422 1.997 1.652] | Epsilon: 0.4975 | Agent-steps: 46 | Length: before 46 after 39 | Ep-max-reward: 0.6703 | Ep-min-reward: 0.3586 | Ep-rnd-reward: 4 0.7294 | Ep-reward: 0.7343 | Ep-max-power: 41.3242 | Ep-min-power: 17.1290 | Ep-rnd-power: 38.9886 | Ep-power: 38.7944 | Num-rrh-on: 2 | Ep-action: [(0, 22), (1, 1), (2, 4), (4, 3), (5, 2), (6, 2), (7, 4), (9, 3), (10, 2), (11, 1), (12, 2)] | Init-state: 1-1-1-1-1-1 | Final-state: 0-1-1-0-0-0
| Episode: 40 | Demand: [1.519 1.209 1.781] | Epsilon: 0.4951 | Agent-steps: 87 | Length: before 41 after 30 | Ep-max-reward: 0.6715 | Ep-min-reward: 0.6151 | Ep-rnd-reward: 3 0.7186 | Ep-reward: 0.6912 | Ep-max-power: 41.2774 | Ep-min-power: 29.9423 | Ep-rnd-power: 39.4146 | Ep-power: 40.4977 | Num-rrh-on: 3 | Ep-action: [(0, 16), (1, 3), (2, 1), (3, 1), (4, 5), (6, 3), (7, 4), (8, 2), (9, 2), (11, 2), (12, 2)] | Init-state: 1-1-1-1-1-1 | Final-state: 1-0-1-0-1-0
| Episode: 60 | Demand: [1.515 1.03  1.074] | Epsilon: 0.4926 | Agent-steps: 135 | Length: before 48 after 41 | Ep-max-reward: 0.6725 | Ep-min-reward: 0.7362 | Ep-rnd-reward: 3 0.7461 | Ep-reward: 0.7342 | Ep-max-power: 41.2349 | Ep-min-power: 33.7590 | Ep-rnd-power: 38.3291 | Ep-power: 38.8002 | Num-rrh-on: 4 | Ep-action: [(0, 19), (1, 2), (2, 1), (3, 2), (4, 6), (5, 2), (6, 3), (7, 1), (8, 5), (9, 3), (10, 2), (11, 1), (12, 1)] | Init-state: 1-1-1-1-1-1 | Final-state: 1-0-1-1-1-0
| Episode: 80 | Demand: [1.273 1.127 1.403] | Epsilon: 0.4902 | Agent-steps: 177 | Length: before 42 after 32 | Ep-max-reward: 0.6697 | Ep-min-reward: 0.6390 | Ep-rnd-reward: 3 0.7163 | Ep-reward: 0.7240 | Ep-max-power: 41.3469 | Ep-min-power: 27.7292 | Ep-rnd-power: 39.5077 | Ep-power: 39.2018 | Num-rrh-on: 2 | Ep-action: [(0, 7), (1, 3), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 2), (9, 15), (10, 1), (11, 1), (12, 7)] | Init-state: 1-1-1-1-1-1 | Final-state: 1-0-0-1-0-0
| Episode: 100 | Demand: [1.164 1.69  1.789] | Epsilon: 0.4877 | Agent-steps: 228 | Length: before 51 after 46 | Ep-max-reward: 0.6710 | Ep-min-reward: 0.6066 | Ep-rnd-reward: 3 0.7322 | Ep-reward: 0.7634 | Ep-max-power: 41.2967 | Ep-min-power: 26.1506 | Ep-rnd-power: 38.8774 | Ep-power: 37.6451 | Num-rrh-on: 4 | Ep-action: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 7), (6, 1), (7, 2), (9, 7), (10, 29), (12, 1)] | Init-state: 1-1-1-1-1-1 | Final-state: 0-1-1-1-0-1
