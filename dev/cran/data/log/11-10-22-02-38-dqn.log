band            : 10000000.0
buffer_size     : 100000
demand_max      : 60000000.0
demand_min      : 50000000.0
dir_log         : ./dev/cran/data/log
dir_mod         : ./dev/cran/data/mod
dir_sum         : ./dev/cran/data/sum
episodes        : 100
epochs          : 10
epsilon_final   : 0.01
epsilon_init    : 0.5
epsilon_steps   : 4000
eta             : 0.25
gamma           : 0.99
load_id         : None
lr              : 0.001
mini_batch      : 64
num_rrh         : 6
num_usr         : 3
observations    : 100
pow_gap         : 3.0
pow_on          : 6.8
pow_slp         : 4.3
pow_tsm         : 1.0
random_seed     : 10000
run_id          : 11-10-22-02-38
save_ep         : 20
tests           : 10
theta_2         : 6.3095734448e-14
tm              : 1
update          : 8
| Episode: 20 | Demand: [5.422 5.997 5.652] | Epsilon: 0.4975 | Agent-steps: 23 | Length: before 23 after 3 | Ep-max-reward: 0.1873 | Ep-min-reward: 0.0000 | Ep-rnd-reward: 6 0.1873 | Ep-reward: 0.1873 | Ep-max-power: 60.4022 | Ep-min-power: 0.0000 | Ep-rnd-power: 60.4022 | Ep-power: 60.4022 | Num-rrh-on: 2 | Ep-action: [(2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 11)] | Init-state: 1-1-1-1-1-1 | Final-state: 1-0-1-0-0-0
| Episode: 40 | Demand: [5.519 5.209 5.781] | Epsilon: 0.4951 | Agent-steps: 50 | Length: before 27 after 7 | Ep-max-reward: 0.2567 | Ep-min-reward: 0.0000 | Ep-rnd-reward: 6 0.2567 | Ep-reward: 0.2567 | Ep-max-power: 57.6620 | Ep-min-power: 0.0000 | Ep-rnd-power: 57.6620 | Ep-power: 57.6620 | Num-rrh-on: 4 | Ep-action: [(0, 1), (1, 3), (2, 1), (4, 4), (6, 2), (7, 2), (8, 1), (10, 3), (12, 10)] | Init-state: 1-1-1-1-1-1 | Final-state: 0-1-1-0-1-1
| Episode: 60 | Demand: [5.515 5.03  5.074] | Epsilon: 0.4926 | Agent-steps: 70 | Length: before 20 after 0 | Ep-max-reward: nan | Ep-min-reward: nan | Ep-rnd-reward: 6 nan | Ep-reward: nan | Ep-max-power: nan | Ep-min-power: nan | Ep-rnd-power: nan | Ep-power: nan | Num-rrh-on: 2 | Ep-action: [(0, 1), (1, 4), (3, 4), (4, 2), (6, 1), (7, 3), (8, 1), (9, 2), (11, 2)] | Init-state: 1-1-1-1-1-1 | Final-state: 0-0-1-1-0-0
| Episode: 80 | Demand: [5.273 5.127 5.403] | Epsilon: 0.4902 | Agent-steps: 90 | Length: before 20 after 0 | Ep-max-reward: nan | Ep-min-reward: nan | Ep-rnd-reward: 6 nan | Ep-reward: nan | Ep-max-power: nan | Ep-min-power: nan | Ep-rnd-power: nan | Ep-power: nan | Num-rrh-on: 4 | Ep-action: [(0, 1), (1, 4), (2, 2), (3, 2), (4, 1), (6, 2), (7, 1), (10, 2), (12, 5)] | Init-state: 1-1-1-1-1-1 | Final-state: 0-0-1-1-1-1
| Episode: 100 | Demand: [5.164 5.69  5.789] | Epsilon: 0.4877 | Agent-steps: 119 | Length: before 29 after 10 | Ep-max-reward: 0.2494 | Ep-min-reward: 0.0000 | Ep-rnd-reward: 6 0.2494 | Ep-reward: 0.2494 | Ep-max-power: 57.9501 | Ep-min-power: 0.0000 | Ep-rnd-power: 57.9501 | Ep-power: 57.9501 | Num-rrh-on: 4 | Ep-action: [(3, 2), (4, 2), (6, 1), (7, 1), (8, 1), (9, 2), (12, 20)] | Init-state: 1-1-1-1-1-1 | Final-state: 1-0-1-1-0-1
